{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14364,
     "status": "ok",
     "timestamp": 1734085878177,
     "user": {
      "displayName": "LEE MING YI",
      "userId": "09779314192865036796"
     },
     "user_tz": -480
    },
    "id": "RCodk8uHObwk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Pima Indians Diabetes dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "df = pd.read_csv(url, names=columns)\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Normalize the feature data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # Make it a column vector\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "diabetes_data_copy = diabetes_df.copy(deep = True)\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X = pd.DataFrame(sc_X.fit_transform(diabetes_data_copy.drop([\"Outcome\"],axis = 1)),\n",
    "        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "                 'BMI', 'DiabetesPedigreeFunction', 'Age'\n",
    "                 ])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNz8YTy6PfTe"
   },
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        # Input layer: 8 features (input size) -> Hidden Layer 1: 64 neurons\n",
    "        self.fc1 = nn.Linear(8, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)  # Hidden Layer 2: 32 neurons\n",
    "        self.fc3 = nn.Linear(32, 1)   # Output layer: 1 neuron for binary classification\n",
    "\n",
    "        # Sigmoid activation for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation to fc1\n",
    "        x = torch.softmax(self.fc2(x))  # Apply ReLU activation to fc2\n",
    "        x = self.fc3(x)              # No activation function here (sigmoid applied later)\n",
    "        x = self.sigmoid(x)          # Sigmoid activation for binary output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3095,
     "status": "ok",
     "timestamp": 1733929413661,
     "user": {
      "displayName": "AIZHAN MAKSATBEK KYZY",
      "userId": "15331045970303564014"
     },
     "user_tz": -480
    },
    "id": "9sSBcuj7PhtN",
    "outputId": "dd5462dd-402e-4181-af79-aece18b800f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1500], Loss: 0.6374\n",
      "Epoch [20/1500], Loss: 0.5956\n",
      "Epoch [30/1500], Loss: 0.5527\n",
      "Epoch [40/1500], Loss: 0.5138\n",
      "Epoch [50/1500], Loss: 0.4836\n",
      "Epoch [60/1500], Loss: 0.4624\n",
      "Epoch [70/1500], Loss: 0.4482\n",
      "Epoch [80/1500], Loss: 0.4386\n",
      "Epoch [90/1500], Loss: 0.4308\n",
      "Epoch [100/1500], Loss: 0.4236\n",
      "Epoch [110/1500], Loss: 0.4169\n",
      "Epoch [120/1500], Loss: 0.4106\n",
      "Epoch [130/1500], Loss: 0.4046\n",
      "Epoch [140/1500], Loss: 0.3987\n",
      "Epoch [150/1500], Loss: 0.3930\n",
      "Epoch [160/1500], Loss: 0.3874\n",
      "Epoch [170/1500], Loss: 0.3819\n",
      "Epoch [180/1500], Loss: 0.3764\n",
      "Epoch [190/1500], Loss: 0.3707\n",
      "Epoch [200/1500], Loss: 0.3649\n",
      "Epoch [210/1500], Loss: 0.3590\n",
      "Epoch [220/1500], Loss: 0.3529\n",
      "Epoch [230/1500], Loss: 0.3466\n",
      "Epoch [240/1500], Loss: 0.3401\n",
      "Epoch [250/1500], Loss: 0.3333\n",
      "Epoch [260/1500], Loss: 0.3261\n",
      "Epoch [270/1500], Loss: 0.3184\n",
      "Epoch [280/1500], Loss: 0.3105\n",
      "Epoch [290/1500], Loss: 0.3024\n",
      "Epoch [300/1500], Loss: 0.2943\n",
      "Epoch [310/1500], Loss: 0.2859\n",
      "Epoch [320/1500], Loss: 0.2771\n",
      "Epoch [330/1500], Loss: 0.2683\n",
      "Epoch [340/1500], Loss: 0.2594\n",
      "Epoch [350/1500], Loss: 0.2504\n",
      "Epoch [360/1500], Loss: 0.2413\n",
      "Epoch [370/1500], Loss: 0.2321\n",
      "Epoch [380/1500], Loss: 0.2228\n",
      "Epoch [390/1500], Loss: 0.2133\n",
      "Epoch [400/1500], Loss: 0.2041\n",
      "Epoch [410/1500], Loss: 0.1947\n",
      "Epoch [420/1500], Loss: 0.1854\n",
      "Epoch [430/1500], Loss: 0.1763\n",
      "Epoch [440/1500], Loss: 0.1675\n",
      "Epoch [450/1500], Loss: 0.1585\n",
      "Epoch [460/1500], Loss: 0.1498\n",
      "Epoch [470/1500], Loss: 0.1414\n",
      "Epoch [480/1500], Loss: 0.1332\n",
      "Epoch [490/1500], Loss: 0.1254\n",
      "Epoch [500/1500], Loss: 0.1178\n",
      "Epoch [510/1500], Loss: 0.1105\n",
      "Epoch [520/1500], Loss: 0.1036\n",
      "Epoch [530/1500], Loss: 0.0970\n",
      "Epoch [540/1500], Loss: 0.0909\n",
      "Epoch [550/1500], Loss: 0.0851\n",
      "Epoch [560/1500], Loss: 0.0796\n",
      "Epoch [570/1500], Loss: 0.0745\n",
      "Epoch [580/1500], Loss: 0.0696\n",
      "Epoch [590/1500], Loss: 0.0650\n",
      "Epoch [600/1500], Loss: 0.0607\n",
      "Epoch [610/1500], Loss: 0.0565\n",
      "Epoch [620/1500], Loss: 0.0527\n",
      "Epoch [630/1500], Loss: 0.0491\n",
      "Epoch [640/1500], Loss: 0.0459\n",
      "Epoch [650/1500], Loss: 0.0430\n",
      "Epoch [660/1500], Loss: 0.0402\n",
      "Epoch [670/1500], Loss: 0.0377\n",
      "Epoch [680/1500], Loss: 0.0354\n",
      "Epoch [690/1500], Loss: 0.0332\n",
      "Epoch [700/1500], Loss: 0.0312\n",
      "Epoch [710/1500], Loss: 0.0293\n",
      "Epoch [720/1500], Loss: 0.0276\n",
      "Epoch [730/1500], Loss: 0.0261\n",
      "Epoch [740/1500], Loss: 0.0246\n",
      "Epoch [750/1500], Loss: 0.0233\n",
      "Epoch [760/1500], Loss: 0.0221\n",
      "Epoch [770/1500], Loss: 0.0209\n",
      "Epoch [780/1500], Loss: 0.0198\n",
      "Epoch [790/1500], Loss: 0.0188\n",
      "Epoch [800/1500], Loss: 0.0179\n",
      "Epoch [810/1500], Loss: 0.0170\n",
      "Epoch [820/1500], Loss: 0.0162\n",
      "Epoch [830/1500], Loss: 0.0155\n",
      "Epoch [840/1500], Loss: 0.0148\n",
      "Epoch [850/1500], Loss: 0.0141\n",
      "Epoch [860/1500], Loss: 0.0135\n",
      "Epoch [870/1500], Loss: 0.0129\n",
      "Epoch [880/1500], Loss: 0.0124\n",
      "Epoch [890/1500], Loss: 0.0118\n",
      "Epoch [900/1500], Loss: 0.0113\n",
      "Epoch [910/1500], Loss: 0.0109\n",
      "Epoch [920/1500], Loss: 0.0104\n",
      "Epoch [930/1500], Loss: 0.0100\n",
      "Epoch [940/1500], Loss: 0.0096\n",
      "Epoch [950/1500], Loss: 0.0093\n",
      "Epoch [960/1500], Loss: 0.0089\n",
      "Epoch [970/1500], Loss: 0.0086\n",
      "Epoch [980/1500], Loss: 0.0083\n",
      "Epoch [990/1500], Loss: 0.0080\n",
      "Epoch [1000/1500], Loss: 0.0077\n",
      "Epoch [1010/1500], Loss: 0.0074\n",
      "Epoch [1020/1500], Loss: 0.0071\n",
      "Epoch [1030/1500], Loss: 0.0069\n",
      "Epoch [1040/1500], Loss: 0.0066\n",
      "Epoch [1050/1500], Loss: 0.0064\n",
      "Epoch [1060/1500], Loss: 0.0062\n",
      "Epoch [1070/1500], Loss: 0.0060\n",
      "Epoch [1080/1500], Loss: 0.0058\n",
      "Epoch [1090/1500], Loss: 0.0056\n",
      "Epoch [1100/1500], Loss: 0.0054\n",
      "Epoch [1110/1500], Loss: 0.0053\n",
      "Epoch [1120/1500], Loss: 0.0051\n",
      "Epoch [1130/1500], Loss: 0.0050\n",
      "Epoch [1140/1500], Loss: 0.0048\n",
      "Epoch [1150/1500], Loss: 0.0047\n",
      "Epoch [1160/1500], Loss: 0.0045\n",
      "Epoch [1170/1500], Loss: 0.0044\n",
      "Epoch [1180/1500], Loss: 0.0043\n",
      "Epoch [1190/1500], Loss: 0.0042\n",
      "Epoch [1200/1500], Loss: 0.0041\n",
      "Epoch [1210/1500], Loss: 0.0039\n",
      "Epoch [1220/1500], Loss: 0.0038\n",
      "Epoch [1230/1500], Loss: 0.0037\n",
      "Epoch [1240/1500], Loss: 0.0036\n",
      "Epoch [1250/1500], Loss: 0.0035\n",
      "Epoch [1260/1500], Loss: 0.0035\n",
      "Epoch [1270/1500], Loss: 0.0034\n",
      "Epoch [1280/1500], Loss: 0.0033\n",
      "Epoch [1290/1500], Loss: 0.0032\n",
      "Epoch [1300/1500], Loss: 0.0031\n",
      "Epoch [1310/1500], Loss: 0.0031\n",
      "Epoch [1320/1500], Loss: 0.0030\n",
      "Epoch [1330/1500], Loss: 0.0029\n",
      "Epoch [1340/1500], Loss: 0.0028\n",
      "Epoch [1350/1500], Loss: 0.0028\n",
      "Epoch [1360/1500], Loss: 0.0027\n",
      "Epoch [1370/1500], Loss: 0.0027\n",
      "Epoch [1380/1500], Loss: 0.0026\n",
      "Epoch [1390/1500], Loss: 0.0025\n",
      "Epoch [1400/1500], Loss: 0.0025\n",
      "Epoch [1410/1500], Loss: 0.0024\n",
      "Epoch [1420/1500], Loss: 0.0024\n",
      "Epoch [1430/1500], Loss: 0.0023\n",
      "Epoch [1440/1500], Loss: 0.0023\n",
      "Epoch [1450/1500], Loss: 0.0022\n",
      "Epoch [1460/1500], Loss: 0.0022\n",
      "Epoch [1470/1500], Loss: 0.0021\n",
      "Epoch [1480/1500], Loss: 0.0021\n",
      "Epoch [1490/1500], Loss: 0.0020\n",
      "Epoch [1500/1500], Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP model, loss function, and optimizer\n",
    "model = MLPModel()\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1500\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1733929413661,
     "user": {
      "displayName": "AIZHAN MAKSATBEK KYZY",
      "userId": "15331045970303564014"
     },
     "user_tz": -480
    },
    "id": "dIAD1PXqPmgs",
    "outputId": "166786f6-500d-43f4-dcb2-24af649dfb48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test set: 66.88%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    predictions = model(X_test)\n",
    "    predicted_labels = (predictions > 0.5).float()  # Convert outputs to 0 or 1 using 0.5 threshold\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    print(f\"Model accuracy on test set: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multulayer Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dset, batch_size):\n",
    "  X_train = dset[columns]\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    X = torch.tensor(X_train.values, dtype=torch.float32).cuda()\n",
    "    y = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).cuda()\n",
    "  else:\n",
    "    X = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "  return DataLoader(list(zip(X, y)), shuffle=True, batch_size=batch_size)\n",
    "\n",
    "train_loader = create_dataloader(X_train, 64)\n",
    "test_loader = create_dataloader(X_test, len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, frst_hidden_neurons, scnd_hidden_neurons, output_size):\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        self.first_hidden_layer = torch.nn.Linear(input_size, frst_hidden_neurons)\n",
    "        self.second_hidden_layer = torch.nn.Linear(frst_hidden_neurons, scnd_hidden_neurons)\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(scnd_hidden_neurons, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        frst_hidden_output = torch.relu(self.first_hidden_layer(x))\n",
    "        scnd_hidden_output = torch.softmax(self.second_hidden_layer(frst_hidden_output), dim=1)\n",
    "\n",
    "        y_pred = torch.sigmoid(self.output_layer(scnd_hidden_output))\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "model = MultiLayerNet(8, 16, 16, 1)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "eval_loss = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(X_batch)\n",
    "      loss = criterion(outputs, y_batch)\n",
    "      loss.backward()\n",
    "\n",
    "      # update parameters\n",
    "      optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "    training_loss.append(loss.item())\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    for x__val_batch, y__val_batch in test_loader:\n",
    "      # model.eval()\n",
    "      val_outputs = model(x__val_batch)\n",
    "      val_loss = criterion(val_outputs, y__val_batch)\n",
    "\n",
    "      acc = (val_outputs.round() == y__val_batch).float().mean()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"Model accuracy: %.2f%%\" % (float(acc)*100))\n",
    "      print('epoch {}, val_loss {}\\n'.format(epoch, val_loss.item()))\n",
    "\n",
    "\n",
    "    eval_loss.append(val_loss.item())\n",
    "    acc_list.append(float(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_loss, linestyle='-', color='b', label='Training Loss')\n",
    "plt.plot(eval_loss, linestyle=':', color='r', label='Validation Loss')\n",
    "plt.plot(acc_list, linestyle='-', color='g', label='Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('Training Progress')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "# Show legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdvancedMultiLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_sizes=[256, 128, 64, 32], output_size=1, dropout_prob=0.4, weight_decay=1e-5):\n",
    "        super(AdvancedMultiLayerNet, self).__init__()\n",
    "\n",
    "        # Dynamically create hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        input_channels = input_size\n",
    "        \n",
    "        # Create hidden layers based on the hidden_layer_sizes list\n",
    "        for size in hidden_layer_sizes:\n",
    "            self.hidden_layers.append(nn.Linear(input_channels, size))\n",
    "            input_channels = size\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(input_channels, output_size)\n",
    "\n",
    "        # Batch Normalization and Dropout layers\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        \n",
    "        for size in hidden_layer_sizes:\n",
    "            self.batch_norms.append(nn.BatchNorm1d(size))\n",
    "            self.dropouts.append(nn.Dropout(dropout_prob))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layers with ReLU, BatchNorm, Dropout, and LeakyReLU\n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            x = hidden_layer(x)\n",
    "            x = self.batch_norms[i](x)  # Apply Batch Normalization\n",
    "            x = F.leaky_relu(x, negative_slope=0.01)  # Use LeakyReLU instead of ReLU\n",
    "            x = self.dropouts[i](x)  # Apply Dropout\n",
    "        \n",
    "        # Output layer with sigmoid activation for binary classification\n",
    "        y_pred = torch.sigmoid(self.output_layer(x))\n",
    "        return y_pred\n",
    "\n",
    "epochs = 3000\n",
    "learningRate = 0.01\n",
    "\n",
    "model = AdvancedMultiLayerNet(input_size=8, hidden_layer_sizes=[256, 128, 64, 32])\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
